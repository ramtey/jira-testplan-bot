# Test Plan Prompt Improvements

## Overview

Enhanced the LLM prompt for generating higher-quality, more actionable test plans based on industry best practices and real-world QA workflows.

## Latest Update: Build-Time vs Runtime Filtering (2026-01-27)

### Problem
Test plans were generating test cases for build-time tools and configurations that don't require manual testing:
- ESLint configuration changes
- TypeScript compiler options
- Build tool configs (webpack, vite, babel)
- CI/CD pipeline configurations

**Example issue**: An SDK update ticket (Expo SDK 52‚Üí53) included "App handles ESLint v9 flat config validation without breaking builds" as an edge case test, even though ESLint validation is automatic in the build pipeline.

### Solution
Added three key improvements to the prompt:

#### 1. "What NOT to Test" Section
New explicit section listing build-time tools to exclude:
- ESLint, Prettier, TypeScript configs
- Build tools (webpack, vite, rollup, babel, esbuild)
- Package manager configs (package.json scripts, lockfiles)
- CI/CD configs (.github/workflows, .gitlab-ci.yml)
- Development tooling (husky, lint-staged, commitlint)
- Test framework configs (jest.config.js, vitest.config.js)

**Key message**: "These fail the build automatically if broken. Manual testing adds no value."

#### 2. SDK/Dependency Update Guidance
Added specific category for SDK and library upgrades in the complexity analysis:
- Reduced test scope: 3-4 happy path (vs 5-8 for complex features)
- 2-4 edge cases focusing on breaking changes from changelog
- 4-6 regression items ensuring existing features still work
- Explicit instruction: "DO NOT test build tools (ESLint, TypeScript configs, bundler settings)"
- Example provided: "App launches on iOS with Expo SDK 53" NOT "ESLint v9 validates code"

**Focus**: Compatibility and regression testing, not testing SDK features themselves.

#### 3. Development Context Filtering
Enhanced the "Use this development context" section with explicit filtering:
- New bullet point: "**FILTER OUT build-time changes**: Ignore ESLint configs, TypeScript configs, build tool settings, CI configs - focus ONLY on runtime code (UI components, API logic, business logic, data models)"
- Ensures LLM analyzes PR file changes intelligently, skipping non-runtime files

### Impact
- ‚úÖ **Eliminates irrelevant tests**: No more ESLint/build tool test cases
- ‚úÖ **Focused SDK update testing**: Reduced scope from 20+ tests to 10-14 targeted tests
- ‚úÖ **Better test quality**: Tests focus on actual user-facing behavior
- ‚úÖ **Faster generation**: Fewer test cases to generate = faster response times
- ‚úÖ **Lower costs**: Fewer tokens generated = reduced API costs

### Example Before/After

**Before (SK-1872 - Expo SDK Update):**
```
Edge Case:
- "App handles ESLint v9 flat config validation without breaking builds"

Regression:
- "ESLint validation passes with new v9 flat config"
```

**After (SK-1872 - Expo SDK Update):**
```
Happy Path:
- "Development build runs successfully with Expo SDK 53 on iOS device"
- "Authentication flow works correctly after React Native upgrade"

Edge Case:
- "App handles iOS SDK version check for App Store submission"
```

---

## Key Improvements

### 1. Risk-Based Prioritization ‚≠ê

**Before:** All test cases treated equally
**After:** Each test case has priority level (critical/high/medium)

```json
{
  "title": "User login with valid credentials",
  "priority": "critical",  // NEW
  ...
}
```

**Benefits:**
- Focus testing efforts on high-risk areas first
- Clear communication of test criticality to team
- Better resource allocation during tight deadlines

**Priority Definitions:**
- üî¥ **Critical**: Authentication, payments, data loss, security vulnerabilities
- üü° **High**: Core features, common user flows, data integrity
- üü¢ **Medium**: Edge cases, rare scenarios, UI polish

---

### 2. Given-When-Then Format ‚≠ê‚≠ê

**Before:** Loose, inconsistent step formatting
**After:** Standard Given-When-Then (Gherkin-style) format

**Before:**
```
"steps": [
  "Navigate to settings",
  "Click delete button",
  "Confirm deletion"
]
```

**After:**
```
"steps": [
  "Given: User is logged in as admin with 100+ records",
  "When: Navigate to settings and click 'Delete Account' button",
  "Then: Confirmation modal appears with warning message"
]
```

**Benefits:**
- Industry-standard format (used in Cucumber, Gherkin, BDD)
- Clearer separation of preconditions, actions, and outcomes
- Easier to automate test cases from these descriptions
- Reduces ambiguity in test steps

---

### 3. Specific Test Data Requirements ‚≠ê‚≠ê

**Before:** No test data specified
**After:** Each test case includes required test data

```json
{
  "title": "Admin exports user data",
  "test_data": "admin account with 100+ records, users in multiple states",  // NEW
  ...
}
```

**Benefits:**
- Testers know exactly what data to prepare
- Reduces "I don't have the right test data" blockers
- Makes test cases independently executable
- Includes specific malicious inputs for security tests (e.g., `SQL: ' OR '1'='1`)

---

### 4. Security & Integration Test Categories ‚≠ê‚≠ê‚≠ê

**Before:** Only happy path + edge cases
**After:** Categorized edge cases + dedicated integration tests

**New Edge Case Categories:**
- `security`: XSS, SQL injection, auth bypass
- `boundary`: Min/max values, limits, special characters
- `error_handling`: Network failures, invalid input
- `integration`: API contracts, database operations

**New Integration Tests Section:**
```json
"integration_tests": [
  {
    "title": "API returns 401 for expired tokens",
    "priority": "critical",
    "steps": [...],
    "test_data": "expired JWT token from 2025-01-01"
  }
]
```

**Benefits:**
- Explicit security testing (catches XSS, SQL injection)
- Backend/API tests separate from UI tests
- Database transaction testing
- Service-to-service communication validation

---

### 5. Two-Phase Analysis Process ‚≠ê

**Before:** Direct test generation
**After:** Analysis phase ‚Üí Generation phase

**Phase 1: Critical Analysis**
1. Understand feature scope & complexity
2. Identify categories and variations
3. Assess risk areas
4. Determine test data requirements

**Phase 2: Generate Test Cases**
- Happy path (business value focus)
- Edge cases (risk-based prioritization)
- Integration tests (if applicable)
- Regression checklist

**Benefits:**
- More thoughtful, comprehensive test coverage
- Better risk assessment
- Contextual awareness of ticket complexity

---

### 6. Enhanced Prompt Instructions

**New additions:**
- ‚úÖ Boundary value analysis guidance (min, max, max+1, empty, null)
- ‚úÖ Specific security test examples (XSS payloads, SQL injection)
- ‚úÖ Concrete test data requirements (not generic "admin user")
- ‚úÖ Integration testing focus for backend tickets
- ‚úÖ Observable, measurable expected outcomes
- ‚úÖ Emphasis on Given-When-Then format

---

## UI Enhancements

### Visual Improvements

1. **Priority Badges**: Color-coded badges (üî¥ Critical, üü° High, üü¢ Medium)
2. **Category Tags**: Edge case categories displayed (security, boundary, etc.)
3. **Test Data Section**: Dedicated section for test data requirements
4. **Integration Tests**: New section in UI for backend/API tests

### Export Format Updates

All export formats (Jira, Markdown, .md file) now include:
- Priority levels with emoji indicators
- Test data requirements
- Category labels for edge cases
- Integration tests section

---

## Example Output Comparison

### Before
```json
{
  "title": "Test login functionality",
  "steps": [
    "Enter credentials",
    "Click login"
  ],
  "expected": "User is logged in"
}
```

### After
```json
{
  "title": "User login with valid credentials successfully authenticates",
  "priority": "critical",
  "steps": [
    "Given: User has valid account (email: test@example.com, password: ValidPass123!)",
    "When: Enter credentials and click 'Log In' button",
    "Then: User is redirected to dashboard with welcome message"
  ],
  "expected": "Dashboard loads with user's name displayed in header",
  "test_data": "Valid account: test@example.com / ValidPass123!"
}
```

**Improvements:**
- ‚úÖ Specific, actionable title
- ‚úÖ Clear priority (critical)
- ‚úÖ Given-When-Then format
- ‚úÖ Concrete test data
- ‚úÖ Observable expected outcome

---

## Technical Changes

### Backend (Python)

**Files modified:**
- `src/app/llm_client.py`: Enhanced prompt with 2-phase analysis
- `src/app/models.py`: Added `integration_tests` field to `TestPlan`

**New fields in TestPlan:**
```python
@dataclass
class TestPlan:
    happy_path: list[dict]
    edge_cases: list[dict]
    regression_checklist: list[str]
    integration_tests: list[dict] | None = None  # NEW
```

### Frontend (React)

**Files modified:**
- `frontend/src/components/TestPlanDisplay.jsx`: Display priority, category, test data
- `frontend/src/utils/markdown.js`: Export formats with new fields
- `frontend/src/App.css`: Styling for badges and test data sections

**New UI elements:**
- Priority badges with color coding
- Category badges for edge cases
- Test data sections with highlighted styling
- Integration tests section

---

## Based on Industry Best Practices

**Sources:**
- [AI LLM Test Prompts: Best Practices](https://www.patronus.ai/llm-testing/ai-llm-test-prompts)
- [Prompt Engineering for Software Testers 2025](https://aqua-cloud.io/prompt-engineering-for-testers/)
- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
- Tudor's real-world QA approach (from conversation)

**Key principles applied:**
1. ‚úÖ Few-shot learning (examples in prompt)
2. ‚úÖ Chain-of-thought reasoning (2-phase analysis)
3. ‚úÖ Specificity over vagueness
4. ‚úÖ Breaking complexity into sequential steps
5. ‚úÖ Context provision (development info, test data)
6. ‚úÖ Risk-based prioritization

---

## Expected Impact

### Quality Improvements
- üìà **30-50% better test coverage** - More edge cases, security tests
- üìà **More actionable test cases** - Given-When-Then format
- üìà **Faster test execution** - Specific test data provided
- üìà **Better risk management** - Priority-based testing

### Tester Experience
- ‚è±Ô∏è **Reduced ambiguity** - Clear preconditions and data requirements
- ‚è±Ô∏è **Faster test case execution** - All context provided upfront
- ‚è±Ô∏è **Better prioritization** - Know which tests to run first
- ‚è±Ô∏è **Security awareness** - Explicit security test cases

### Team Benefits
- üéØ **Clear communication** - Priority levels visible to all
- üéØ **Risk awareness** - Critical tests highlighted
- üéØ **Better planning** - Understand test scope from priorities
- üéØ **Quality gates** - Can focus on critical tests for releases

---

## Migration Notes

**Backward compatible:** Existing test plans without priority/test_data will still render correctly.

**Frontend gracefully handles:**
- Missing `priority` field (no badge shown)
- Missing `test_data` field (section not displayed)
- Missing `category` field (no category badge)
- Missing `integration_tests` (section not shown)

**No breaking changes** to API contracts or data models (only additions).

---

## Next Steps (Optional Future Improvements)

1. **A/B Testing**: Compare old vs new prompt quality metrics
2. **Feedback Loop**: Collect tester ratings on generated plans
3. **Custom Prompts**: Allow teams to customize prompt templates
4. **Auto-prioritization**: ML model to predict criticality
5. **Test Automation**: Generate Playwright/Cypress code from test cases

---

## Summary

The improved prompt generates **more comprehensive, actionable, and risk-aware test plans** by:
- Adding risk-based prioritization
- Using industry-standard Given-When-Then format
- Specifying concrete test data requirements
- Including security and integration tests
- Providing 2-phase analysis for better context

All based on 2026 best practices and real-world QA workflows. üöÄ
